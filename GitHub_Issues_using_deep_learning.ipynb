{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GitHub Issues_using_deep_learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyanshgupta1998/Tensorflow_deep_learning/blob/master/GitHub_Issues_using_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98KaeipUhxdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNOPQgynhxaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfc-KJvnHjne",
        "colab_type": "text"
      },
      "source": [
        "#GitHub Issues   \n",
        "GitHub issue titles and descriptions for NLP analysis.\n",
        "\n",
        "Text Processing with S2S"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6M7IEht5yNx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#Data load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oLR3v3TyyWH",
        "colab_type": "code",
        "outputId": "c9a0e52c-0912-478f-ab52-edc32fc0bb29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /home"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G7rWPfByyYD",
        "colab_type": "code",
        "outputId": "de364907-e6fa-4a6c-f7ee-f1d3339acb00",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-acd6c538-5ba6-442d-b16d-8fcf1fc9cf7d\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-acd6c538-5ba6-442d-b16d-8fcf1fc9cf7d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp5zrmZtyybE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!kaggle datasets list -s nlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF5UGEfTyygc",
        "colab_type": "code",
        "outputId": "3c340df3-a41f-4ac3-bc02-cfab9b90df52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle datasets download -d davidshinn/github-issues "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading github-issues.zip to /home\n",
            " 99% 974M/980M [00:19<00:00, 55.5MB/s]\n",
            "100% 980M/980M [00:19<00:00, 51.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JXmjzUfGbaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d432d061-24d7-497b-a1b4-66efbd1174ef"
      },
      "source": [
        "!unzip github-issues.zip"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  github-issues.zip\n",
            "  inflating: github_issues.csv       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzpV6abeIs2S",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GINfJEABBkne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "aa9d4cb1-305a-4e39-8524-843c9cc714d5"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 500)\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "stop = set(stopwords.words('english'))\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import matplotlib as plty\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from pprint import pprint\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)  \n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\"]).decode(\"utf8\"))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/vnd.plotly.v1+html": "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>",
            "text/html": [
              "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "github_issues.csv\n",
            "github-issues.zip\n",
            "kaggle.json\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxqFgKRTj4Zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #!pip install pyLDAvis\n",
        "# import pyLDAvis    # Topic modeling\n",
        "# import pyLDAvis.gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7-yNVbTJQkL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0d5eb48-d20a-4755-9889-8db8c6ed2e8e"
      },
      "source": [
        "#!pip install ktext\n",
        "from ktext.preprocess import processor"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq-9w6dmJ_mL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "46dd1ac1-0dd0-477a-c248-22f88ce83dab"
      },
      "source": [
        "df = pd.read_csv('github_issues.csv')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5332153, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_url</th>\n",
              "      <th>issue_title</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\"https://github.com/zhangyuanwei/node-images/issues/123\"</td>\n",
              "      <td>can't load the addon. issue to: https://github.com/zhangyuanwei/node-images/issues error: /lib64/libc.so.6: version glibc_2.14' not found required by /usr/local/app/taf/fileserver.fileserver/bin/src/node_modules/images/bindings/linux/x64/8.0.0/binding.node</td>\n",
              "      <td>can't load the addon. issue to: https://github.com/zhangyuanwei/node-images/issues error: /lib64/libc.so.6: version glibc_2.14' not found required by /usr/local/app/taf/fileserver.fileserver/bin/src/node_modules/images/bindings/linux/x64/8.0.0/binding.node 有低点的版本，不需要glibc_2.14的吗</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"https://github.com/Microsoft/pxt/issues/2543\"</td>\n",
              "      <td>hcl accessibility a11yblocking a11ymas mas4.2.10 hcl-makecode win10-edge -title screen reader-help-javascript-call a function narrator focus does not moving to expand side a documentation button after pressing enter on collapse side a documentation button.</td>\n",
              "      <td>user experience: user who depends on screen reader will get confused if narrator focus does not retain on expand side a documentation button after pressing enter on collapse side a documentation button test environment os: rs2 version 1703 os build 15063.483 platform: edge. screen reader: narrator repro steps 1-navigate to https://makecode.microbit.org/acc 2-navigate to the micro bit section element and select code control given on it. 3-navigate to the help control lying in the header secti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"https://github.com/MatisiekPL/Czekolada/issues/1268\"</td>\n",
              "      <td>issue 1265: issue 1264: issue 1261: issue 1260: issue 1257: issue 1256: issue 1253: issue 1252: issue 1250: issue 1247: issue 1246: issue 1243: issue 1242: issue 1239: issue 1237: issue 1236: issue 1233: issue 1231: issue 1230: issue 1227: issue 1226: issu</td>\n",
              "      <td>┆attachments: &lt;a href= https:&amp; x2f;&amp; x2f;github.com&amp; x2f;matisiekpl&amp; x2f;czekolada&amp; x2f;issues&amp; x2f;1265 &gt;https:&amp; x2f;&amp; x2f;github.com&amp; x2f;matisiekpl&amp; x2f;czekolada&amp; x2f;issues&amp; x2f;1265&lt;/a&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"https://github.com/MatisiekPL/Czekolada/issues/1267\"</td>\n",
              "      <td>issue 1266: issue 1263: issue 1262: issue 1259: issue 1258: issue 1255: issue 1254: issue 1251: issue 1249: issue 1248: issue 1245: issue 1244: issue 1241: issue 1240: issue 1238: issue 1235: issue 1234: issue 1232: issue 1229: issue 1228: issue 1225: issu</td>\n",
              "      <td>gitlo = github x trello\\n---\\nthis board is now linked with https://github.com/matisiekpl/czekolada , any update on the issue tracker will be sync to this board. -------\\nvia trello, you can: --- __✍ create github issues__ - add a card in the corresponding column, an issue will be created in github ! add an issue http://i.imgur.com/yewicu8.gif ------- __✐ comment on github issues__ - just comment as you always do in a card ! comment on an issue http://i.imgur.com/jdnjscf.gif ------- __➦ clos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"https://github.com/MatisiekPL/Czekolada/issues/1289\"</td>\n",
              "      <td>issue 1288: issue 1285: issue 1284: issue 1281: issue 1280: issue 1277: issue 1276: issue 1273: issue 1272: issue 1269: issue 1268: issue 1265: issue 1264: issue 1261: issue 1260: issue 1257: issue 1256: issue 1253: issue 1252: issue 1250: issue 1247: issu</td>\n",
              "      <td>┆attachments: &lt;a href= https:&amp; x2f;&amp; x2f;github.com&amp; x2f;matisiekpl&amp; x2f;czekolada&amp; x2f;issues&amp; x2f;1288 &gt;https:&amp; x2f;&amp; x2f;github.com&amp; x2f;matisiekpl&amp; x2f;czekolada&amp; x2f;issues&amp; x2f;1288&lt;/a&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  issue_url  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 body\n",
              "0  \"https://github.com/zhangyuanwei/node-images/issues/123\"  ...                                                                                                                                                                                                                              can't load the addon. issue to: https://github.com/zhangyuanwei/node-images/issues error: /lib64/libc.so.6: version glibc_2.14' not found required by /usr/local/app/taf/fileserver.fileserver/bin/src/node_modules/images/bindings/linux/x64/8.0.0/binding.node 有低点的版本，不需要glibc_2.14的吗\n",
              "1            \"https://github.com/Microsoft/pxt/issues/2543\"  ...  user experience: user who depends on screen reader will get confused if narrator focus does not retain on expand side a documentation button after pressing enter on collapse side a documentation button test environment os: rs2 version 1703 os build 15063.483 platform: edge. screen reader: narrator repro steps 1-navigate to https://makecode.microbit.org/acc 2-navigate to the micro bit section element and select code control given on it. 3-navigate to the help control lying in the header secti...\n",
              "2     \"https://github.com/MatisiekPL/Czekolada/issues/1268\"  ...                                                                                                                                                                                                                                                                                                                      ┆attachments: <a href= https:& x2f;& x2f;github.com& x2f;matisiekpl& x2f;czekolada& x2f;issues& x2f;1265 >https:& x2f;& x2f;github.com& x2f;matisiekpl& x2f;czekolada& x2f;issues& x2f;1265</a>\n",
              "3     \"https://github.com/MatisiekPL/Czekolada/issues/1267\"  ...  gitlo = github x trello\\n---\\nthis board is now linked with https://github.com/matisiekpl/czekolada , any update on the issue tracker will be sync to this board. -------\\nvia trello, you can: --- __✍ create github issues__ - add a card in the corresponding column, an issue will be created in github ! add an issue http://i.imgur.com/yewicu8.gif ------- __✐ comment on github issues__ - just comment as you always do in a card ! comment on an issue http://i.imgur.com/jdnjscf.gif ------- __➦ clos...\n",
              "4     \"https://github.com/MatisiekPL/Czekolada/issues/1289\"  ...                                                                                                                                                                                                                                                                                                                      ┆attachments: <a href= https:& x2f;& x2f;github.com& x2f;matisiekpl& x2f;czekolada& x2f;issues& x2f;1288 >https:& x2f;& x2f;github.com& x2f;matisiekpl& x2f;czekolada& x2f;issues& x2f;1288</a>\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFNgJF25JNQy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "579eb2c4-fef9-4dc2-8870-63ba4eb90f6a"
      },
      "source": [
        "#Take data of size 60,000 randomly ( out of dataset of size 53,32,153)\n",
        "traindf, testdf = train_test_split(pd.read_csv('github_issues.csv').sample(n=60000), \n",
        "                                   test_size=.10)\n",
        "print(traindf.shape, testdf.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(54000, 3) (6000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFdNMgzJhzE2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "6ac48644-2919-4a95-bf40-1c6dd18bce24"
      },
      "source": [
        "traindf.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_url</th>\n",
              "      <th>issue_title</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4870664</th>\n",
              "      <td>\"https://github.com/blatinier/RendezMoiMesPlumes/issues/112\"</td>\n",
              "      <td>ability to edit products</td>\n",
              "      <td>- add the possibility for users to edits products if informations is missing and/or is not accurate enough. - enqueue edit in a review system admin validation only to begin with - add some points to the user score when edits are accepted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1930493</th>\n",
              "      <td>\"https://github.com/awslabs/aws-ai-qna-bot/issues/1\"</td>\n",
              "      <td>handler lambda incorrectly parses alexa events</td>\n",
              "      <td>error reported from customer: alexa is logging this error typeerror: cannot read property 'slots' of undefined at module.exports /var/task/lib/parse.js:33:39 i think the problem is that the handler lambda does not parse the alexa launchrequest and sessionendedrequest revents correctly. first, replicate this problem. second, added test cases for these events third, added additional parsing logic to lambda function last, verify fix</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13300</th>\n",
              "      <td>\"https://github.com/APISENSE/android-network-measures/issues/7\"</td>\n",
              "      <td>tokenize on null</td>\n",
              "      <td>after downloading and trying to import as module it gives and error cannot invoke method tokenize on null object. in gradle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3295613</th>\n",
              "      <td>\"https://github.com/HaiyangXu/osm-bundler/issues/5\"</td>\n",
              "      <td>filenotfounderror: errno 2</td>\n",
              "      <td>i'm getting error when running script. i'm using yosemite 10.10.5. please help!! image = image.open '/users/julianjamison/desktop/images_code.jpg' file /library/frameworks/python.framework/versions/3.5/lib/python3.5/site-packages/pil/image.py , line 2410, in open fp = builtins.open filename, rb filenotfounderror: errno 2 no such file or directory: '/users/julianjamison/desktop/images_code.jpg'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2637555</th>\n",
              "      <td>\"https://github.com/DD-DeCaF/decaf-frontend/issues/9\"</td>\n",
              "      <td>as user i want to land on an apologetic landing page if the decaf platform is down</td>\n",
              "      <td>you know like the github unicorn page for example https://thenextweb.com/wp-content/blogs.dir/1/files/2013/06/unicorn.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                               issue_url  ...                                                                                                                                                                                                                                                                                                                                                                                                                                               body\n",
              "4870664     \"https://github.com/blatinier/RendezMoiMesPlumes/issues/112\"  ...                                                                                                                                                                                                      - add the possibility for users to edits products if informations is missing and/or is not accurate enough. - enqueue edit in a review system admin validation only to begin with - add some points to the user score when edits are accepted\n",
              "1930493             \"https://github.com/awslabs/aws-ai-qna-bot/issues/1\"  ...  error reported from customer: alexa is logging this error typeerror: cannot read property 'slots' of undefined at module.exports /var/task/lib/parse.js:33:39 i think the problem is that the handler lambda does not parse the alexa launchrequest and sessionendedrequest revents correctly. first, replicate this problem. second, added test cases for these events third, added additional parsing logic to lambda function last, verify fix\n",
              "13300    \"https://github.com/APISENSE/android-network-measures/issues/7\"  ...                                                                                                                                                                                                                                                                                                                        after downloading and trying to import as module it gives and error cannot invoke method tokenize on null object. in gradle\n",
              "3295613              \"https://github.com/HaiyangXu/osm-bundler/issues/5\"  ...                                       i'm getting error when running script. i'm using yosemite 10.10.5. please help!! image = image.open '/users/julianjamison/desktop/images_code.jpg' file /library/frameworks/python.framework/versions/3.5/lib/python3.5/site-packages/pil/image.py , line 2410, in open fp = builtins.open filename, rb filenotfounderror: errno 2 no such file or directory: '/users/julianjamison/desktop/images_code.jpg'\n",
              "2637555            \"https://github.com/DD-DeCaF/decaf-frontend/issues/9\"  ...                                                                                                                                                                                                                                                                                                                          you know like the github unicorn page for example https://thenextweb.com/wp-content/blogs.dir/1/files/2013/06/unicorn.png\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv6q9nPQLAlY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "59c9ea23-97ed-4880-ba08-18360743c228"
      },
      "source": [
        "testdf.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>issue_url</th>\n",
              "      <th>issue_title</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>444518</th>\n",
              "      <td>\"https://github.com/maralla/completor.vim/issues/91\"</td>\n",
              "      <td>add support for other javascript filetypes like javascript.jsx</td>\n",
              "      <td>it took me nearly 20mins to figure out why the javascript completion was not working at all when i realized that i set my javascript ft=javascript.jsx &amp; it seems that in order for it to work it ft must be javascript only https://github.com/maralla/completor.vim/blob/master/pythonx/completers/javascript/__init__.py l13 . it will be nice to support other filetypes too.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4940110</th>\n",
              "      <td>\"https://github.com/andrewtelnov/surveyjs/issues/180\"</td>\n",
              "      <td>date field for older browsers</td>\n",
              "      <td>hi andrew - the older browser versions dont support html date field - so how i do achieve this functionality of a date picker in the questions ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4599635</th>\n",
              "      <td>\"https://github.com/rails/strong_parameters/issues/251\"</td>\n",
              "      <td>no way to cleanly express recursive parameters</td>\n",
              "      <td>consider an object which belongs_to :parent and has_many :children, and which wishes to accept nested parameters for its children. a few minutes of searching has turned up a lot of here's how to unconditionally whitelist the entire tree rooted at a given point , but of course, that's exactly not what we want; we want to actually have working parameter protection which recognizes that, whenever a nested attribute of this type is found, we accept a specific fixed set of attributes for it. even...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2922691</th>\n",
              "      <td>\"https://github.com/joshcummingsdesign/grizzly-wp/issues/115\"</td>\n",
              "      <td>clone staging causes permissions errors</td>\n",
              "      <td>running make clone-staging causes permissions errors noticed when plugins wouldn't update there may be a better way to pull plugins, uploads, etc. also may have to run permissions fix afterwards.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1370820</th>\n",
              "      <td>\"https://github.com/rebus-org/Rebus/issues/595\"</td>\n",
              "      <td>question is rebus good for my requirements?</td>\n",
              "      <td>scenario i have two asp.net applicazioni: first one is a web api and the second one a regular mvc web site. i have the following requirements: 1. from the web api i need to raise some events that will be offloaded to the mvc web site for compute 2. the messages exchanged should be threated as transactional. that is, the message should be considered handled only if the receiver confirm the handling 3. messages should be either synchronous as well as asynchronous i am using ninject in both sol...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                             issue_url  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 body\n",
              "444518            \"https://github.com/maralla/completor.vim/issues/91\"  ...                                                                                                                                    it took me nearly 20mins to figure out why the javascript completion was not working at all when i realized that i set my javascript ft=javascript.jsx & it seems that in order for it to work it ft must be javascript only https://github.com/maralla/completor.vim/blob/master/pythonx/completers/javascript/__init__.py l13 . it will be nice to support other filetypes too.\n",
              "4940110          \"https://github.com/andrewtelnov/surveyjs/issues/180\"  ...                                                                                                                                                                                                                                                                                                                                                                     hi andrew - the older browser versions dont support html date field - so how i do achieve this functionality of a date picker in the questions ?\n",
              "4599635        \"https://github.com/rails/strong_parameters/issues/251\"  ...  consider an object which belongs_to :parent and has_many :children, and which wishes to accept nested parameters for its children. a few minutes of searching has turned up a lot of here's how to unconditionally whitelist the entire tree rooted at a given point , but of course, that's exactly not what we want; we want to actually have working parameter protection which recognizes that, whenever a nested attribute of this type is found, we accept a specific fixed set of attributes for it. even...\n",
              "2922691  \"https://github.com/joshcummingsdesign/grizzly-wp/issues/115\"  ...                                                                                                                                                                                                                                                                                                                  running make clone-staging causes permissions errors noticed when plugins wouldn't update there may be a better way to pull plugins, uploads, etc. also may have to run permissions fix afterwards.\n",
              "1370820                \"https://github.com/rebus-org/Rebus/issues/595\"  ...  scenario i have two asp.net applicazioni: first one is a web api and the second one a regular mvc web site. i have the following requirements: 1. from the web api i need to raise some events that will be offloaded to the mvc web site for compute 2. the messages exchanged should be threated as transactional. that is, the message should be considered handled only if the receiver confirm the handling 3. messages should be either synchronous as well as asynchronous i am using ninject in both sol...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUvDpPxFLgOY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "f6530c70-b4db-485a-d590-18707258eda4"
      },
      "source": [
        "train_body_raw = traindf.body.tolist()\n",
        "print(len(train_body_raw))\n",
        "train_body_raw[:5]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['- add the possibility for users to edits products if informations is missing and/or is not accurate enough. - enqueue edit in a review system admin validation only to begin with - add some points to the user score when edits are accepted',\n",
              " \"error reported from customer: alexa is logging this error typeerror: cannot read property 'slots' of undefined at module.exports /var/task/lib/parse.js:33:39 i think the problem is that the handler lambda does not parse the alexa launchrequest and sessionendedrequest revents correctly. first, replicate this problem. second, added test cases for these events third, added additional parsing logic to lambda function last, verify fix\",\n",
              " 'after downloading and trying to import as module it gives and error cannot invoke method tokenize on null object. in gradle',\n",
              " \"i'm getting error when running script. i'm using yosemite 10.10.5. please help!! image = image.open '/users/julianjamison/desktop/images_code.jpg' file /library/frameworks/python.framework/versions/3.5/lib/python3.5/site-packages/pil/image.py , line 2410, in open fp = builtins.open filename, rb filenotfounderror: errno 2 no such file or directory: '/users/julianjamison/desktop/images_code.jpg'\",\n",
              " 'you know like the github unicorn page for example https://thenextweb.com/wp-content/blogs.dir/1/files/2013/06/unicorn.png']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYO_YUjDkdgO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "aed2d40b-56da-4490-a4c4-f9ccf88f4760"
      },
      "source": [
        "train_title_raw = traindf.issue_title.tolist()\n",
        "print(len(train_title_raw))\n",
        "train_title_raw[:5]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ability to edit products',\n",
              " 'handler lambda incorrectly parses alexa events',\n",
              " 'tokenize on null',\n",
              " 'filenotfounderror: errno 2',\n",
              " 'as user i want to land on an apologetic landing page if the decaf platform is down']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLEBO66iMNZn",
        "colab_type": "text"
      },
      "source": [
        "#Use ktext to pre-process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CRN9oOmkdm4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "63a71369-f5d4-4514-cb8d-687427671800"
      },
      "source": [
        "num_encoder_tokens = 5500\n",
        "body_pp = processor(keep_n=num_encoder_tokens, \n",
        "                    padding_maxlen=50)\n",
        "train_body_vecs = body_pp.fit_transform(train_body_raw)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:....tokenizing data\n",
            "WARNING:root:(1/2) done. 33 sec\n",
            "WARNING:root:....building corpus\n",
            "WARNING:root:(2/2) done. 2 sec\n",
            "WARNING:root:Finished parsing 54,000 documents.\n",
            "WARNING:root:...fit is finished, beginning transform\n",
            "WARNING:root:...padding data\n",
            "WARNING:root:done. 1 sec\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu3VrTPUkdrZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "4f03a3a7-8ba3-4af2-8c0a-32bb498cb54f"
      },
      "source": [
        "print(train_body_vecs.shape)\n",
        "print('\\noriginal string:\\n', train_body_raw[0], '\\n')\n",
        "print('after pre-processing:\\n', train_body_vecs[0], '\\n')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(54000, 50)\n",
            "\n",
            "original string:\n",
            " - add the possibility for users to edits products if informations is missing and/or is not accurate enough. - enqueue edit in a review system admin validation only to begin with - add some points to the user score when edits are accepted \n",
            "\n",
            "after pre-processing:\n",
            " [   0    0    0    0    0    0    0    0    0   63    3 1988   14  146\n",
            "    4 5129 1049   21 4645    8  323    9   29    8   19 4162 1052    1\n",
            "  570    7    5 1061  121  548 1089   70    4 1507   18   63   66  991\n",
            "    4    3   64 2040   28 5129   33 2456] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYmNZpVDkdz3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "1a852be3-3729-43d2-e1a2-f0f561c0da37"
      },
      "source": [
        "num_decoder_tokens=4500\n",
        "title_pp = processor(append_indicators=True, \n",
        "                     keep_n=num_decoder_tokens, \n",
        "                     padding_maxlen=12, \n",
        "                     padding ='post')\n",
        "\n",
        "# process the title data\n",
        "train_title_vecs = title_pp.fit_transform(train_title_raw)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:....tokenizing data\n",
            "WARNING:root:(1/2) done. 5 sec\n",
            "WARNING:root:....building corpus\n",
            "WARNING:root:(2/2) done. 0 sec\n",
            "WARNING:root:Finished parsing 54,000 documents.\n",
            "WARNING:root:...fit is finished, beginning transform\n",
            "WARNING:root:...padding data\n",
            "WARNING:root:done. 0 sec\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_YlgmtYM9lf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "2344173f-4168-4264-db0b-1599a393a73a"
      },
      "source": [
        "print(train_title_vecs.shape)\n",
        "print('\\noriginal string:\\n', train_title_raw[0], '\\n')\n",
        "print('after pre-processing:\\n', train_title_vecs[0], '\\n')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(54000, 12)\n",
            "\n",
            "original string:\n",
            " ability to edit products \n",
            "\n",
            "after pre-processing:\n",
            " [   2  293    4  277 1074    3    0    0    0    0    0    0] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swPRbM--kdjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input_data = train_body_vecs\n",
        "doc_length = encoder_input_data.shape[1]\n",
        "\n",
        "decoder_input_data = train_title_vecs[:, :-1]\n",
        "decoder_target_data = train_title_vecs[:, 1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsD8z_p7hy6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_encoder_tokens = max(body_pp.id2token.keys()) + 1\n",
        "num_decoder_tokens = max(title_pp.id2token.keys()) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaWU_R3nhy3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PTPbRZ9UdKD",
        "colab_type": "text"
      },
      "source": [
        "#Define Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9tqUe5nVJft",
        "colab_type": "text"
      },
      "source": [
        "###Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2tpom-iUujU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "66fb53e8-1de2-4e49-b261-802d96279f1c"
      },
      "source": [
        "#arbitrarly set latent dimension for embedding and hidden units\n",
        "latent_dim = 80\n",
        "\n",
        "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
        "\n",
        "# Word embeding for encoder (ex: Issue , Body)\n",
        "x = Embedding(num_encoder_tokens, \n",
        "              latent_dim,\n",
        "              name='Body-Word-Embedding', \n",
        "              mask_zero=False)(encoder_inputs)\n",
        "\n",
        "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tQ2P-xbU2So",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Intermediate GRU layer (optional)\n",
        "#x = GRU(latent_dim, name='Encoder-Intermediate-GRU', return_sequences=True)(x)\n",
        "#x = BatchNormalization(name='Encoder-Batchnorm-2')(x)\n",
        "# We do not need the `encoder_output` just the hidden state.\n",
        "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynuNF2lzU-22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Encapsulate the encoder as a separate entity so we can just \n",
        "#  encode without decoding if we want to.\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
        "\n",
        "seq2seq_encoder_out = encoder_model(encoder_inputs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cq5wK8MVFUr",
        "colab_type": "text"
      },
      "source": [
        "###Decoder Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5iHKGqaMDVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
        "\n",
        "# Word Embedding For Decoder (ex: Issue Titles)\n",
        "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
        "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
        "# Set up the decoder, using `decoder_state_input` as initial state.\n",
        "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
        "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
        "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcBPLHWjVVfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dense layer for prediction\n",
        "decoder_dense = Dense(num_decoder_tokens+2, activation='softmax', name='Final-Output-Dense')\n",
        "decoder_outputs = decoder_dense(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASWbkLWhVYEv",
        "colab_type": "text"
      },
      "source": [
        "#Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLAPbXFnVSCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\n",
        "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVTD3xjSVR7i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
        "\n",
        "script_name_base = 'tutorial_seq2seq'\n",
        "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
        "                                   save_best_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nKGgJ92VRoM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "dd535b66-8bb9-411f-ece0-4b1a1ef0e479"
      },
      "source": [
        "batch_size = 100\n",
        "epochs = 4\n",
        "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.10, callbacks=[model_checkpoint])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 48600 samples, validate on 5400 samples\n",
            "Epoch 1/4\n",
            "48600/48600 [==============================] - 42s 857us/step - loss: 4.0605 - val_loss: 3.4796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning:\n",
            "\n",
            "Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 80) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/4\n",
            "48600/48600 [==============================] - 38s 788us/step - loss: 3.3035 - val_loss: 3.2792\n",
            "Epoch 3/4\n",
            "  200/48600 [..............................] - ETA: 34s - loss: 3.1119"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning:\n",
            "\n",
            "Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 80) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "48600/48600 [==============================] - 39s 796us/step - loss: 3.0609 - val_loss: 3.2035\n",
            "Epoch 4/4\n",
            "  200/48600 [..............................] - ETA: 35s - loss: 2.7036"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning:\n",
            "\n",
            "Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 80) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "48600/48600 [==============================] - 39s 798us/step - loss: 2.8636 - val_loss: 3.1863\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning:\n",
            "\n",
            "Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 80) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSQABuxqMD4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMJF35bfWpuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_decoder_model(model):\n",
        "    \"\"\"\n",
        "    Extract the decoder from the original model.\n",
        "    Inputs:\n",
        "    ------\n",
        "    model: keras model object\n",
        "    Returns:\n",
        "    -------\n",
        "    A Keras model object with the following inputs and outputs:\n",
        "    Inputs of Keras Model That Is Returned:\n",
        "    1: the embedding index for the last predicted word or the <Start> indicator\n",
        "    2: the last hidden state, or in the case of the first word the hidden state from the encoder\n",
        "    Outputs of Keras Model That Is Returned:\n",
        "    1.  Prediction (class probabilities) for the next word\n",
        "    2.  The hidden state of the decoder, to be fed back into the decoder at the next time step\n",
        "    Implementation Notes:\n",
        "    ----------------------\n",
        "    Must extract relevant layers and reconstruct part of the computation graph\n",
        "    to allow for different inputs as we are not going to use teacher forcing at\n",
        "    inference time.\n",
        "    \"\"\"\n",
        "    # the latent dimension is the same throughout the architecture so we are going to\n",
        "    # cheat and grab the latent dimension of the embedding because that is the same as what is\n",
        "    # output from the decoder\n",
        "    latent_dim = model.get_layer('Decoder-Word-Embedding').output_shape[-1]\n",
        "\n",
        "    # Reconstruct the input into the decoder\n",
        "    decoder_inputs = model.get_layer('Decoder-Input').input\n",
        "    dec_emb = model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
        "    dec_bn = model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
        "\n",
        "    # Instead of setting the intial state from the encoder and forgetting about it, during inference\n",
        "    # we are not doing teacher forcing, so we will have to have a feedback loop from predictions back into\n",
        "    # the GRU, thus we define this input layer for the state so we can add this capability\n",
        "    gru_inference_state_input = Input(shape=(latent_dim,), name='hidden_state_input')\n",
        "    # we need to reuse the weights that is why we are getting this\n",
        "    # If you inspect the decoder GRU that we created for training, it will take as input\n",
        "    # 2 tensors -> (1) is the embedding layer output for the teacher forcing\n",
        "    #                  (which will now be the last step's prediction, and will be _start_ on the first time step)\n",
        "    #              (2) is the state, which we will initialize with the encoder on the first time step, but then\n",
        "    #                   grab the state after the first prediction and feed that back in again.\n",
        "    gru_out, gru_state_out = model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])\n",
        "\n",
        "    # Reconstruct dense layers\n",
        "    dec_bn2 = model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
        "    dense_out = model.get_layer('Final-Output-Dense')(dec_bn2)\n",
        "    decoder_model = Model([decoder_inputs, gru_inference_state_input],\n",
        "                          [dense_out, gru_state_out])\n",
        "    return decoder_model\n",
        "def extract_encoder_model(model):\n",
        "    \"\"\"\n",
        "    Extract the encoder from the original Sequence to Sequence Model.\n",
        "    Returns a keras model object that has one input (body of issue) and one\n",
        "    output (encoding of issue, which is the last hidden state).\n",
        "    Input:\n",
        "    -----\n",
        "    model: keras model object\n",
        "    Returns:\n",
        "    -----\n",
        "    keras model object\n",
        "    \"\"\"\n",
        "    encoder_model = model.get_layer('Encoder-Model')\n",
        "    return encoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMGItmRxMD2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq_Inference(object):\n",
        "    def __init__(self,\n",
        "                 encoder_preprocessor,\n",
        "                 decoder_preprocessor,\n",
        "                 seq2seq_model):\n",
        "\n",
        "        self.pp_body = encoder_preprocessor\n",
        "        self.pp_title = decoder_preprocessor\n",
        "        self.seq2seq_model = seq2seq_model\n",
        "        self.encoder_model = extract_encoder_model(seq2seq_model)\n",
        "        self.decoder_model = extract_decoder_model(seq2seq_model)\n",
        "        self.default_max_len_title = self.pp_title.padding_maxlen\n",
        "        self.nn = None\n",
        "        self.rec_df = None\n",
        "\n",
        "    def generate_issue_title(self,\n",
        "                             raw_input_text,\n",
        "                             max_len_title=None):\n",
        "        \"\"\"\n",
        "        Use the seq2seq model to generate a title given the body of an issue.\n",
        "        Inputs\n",
        "        ------\n",
        "        raw_input: str\n",
        "            The body of the issue text as an input string\n",
        "        max_len_title: int (optional)\n",
        "            The maximum length of the title the model will generate\n",
        "        \"\"\"\n",
        "        if max_len_title is None:\n",
        "            max_len_title = self.default_max_len_title\n",
        "        # get the encoder's features for the decoder\n",
        "        raw_tokenized = self.pp_body.transform([raw_input_text])\n",
        "        body_encoding = self.encoder_model.predict(raw_tokenized)\n",
        "        # we want to save the encoder's embedding before its updated by decoder\n",
        "        #   because we can use that as an embedding for other tasks.\n",
        "        original_body_encoding = body_encoding\n",
        "        state_value = np.array(self.pp_title.token2id['_start_']).reshape(1, 1)\n",
        "\n",
        "        decoded_sentence = []\n",
        "        stop_condition = False\n",
        "        while not stop_condition:\n",
        "            preds, st = self.decoder_model.predict([state_value, body_encoding])\n",
        "\n",
        "            # We are going to ignore indices 0 (padding) and indices 1 (unknown)\n",
        "            # Argmax will return the integer index corresponding to the\n",
        "            #  prediction + 2 b/c we chopped off first two\n",
        "            pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
        "\n",
        "            # retrieve word from index prediction\n",
        "            pred_word_str = self.pp_title.id2token[pred_idx]\n",
        "            if pred_word_str == '_end_' or len(decoded_sentence) >= max_len_title:\n",
        "                stop_condition = True\n",
        "                break\n",
        "            decoded_sentence.append(pred_word_str)\n",
        "\n",
        "            # update the decoder for the next word\n",
        "            body_encoding = st\n",
        "            state_value = np.array(pred_idx).reshape(1, 1)\n",
        "\n",
        "        return original_body_encoding, ' '.join(decoded_sentence)\n",
        "\n",
        "\n",
        "    def print_example(self,\n",
        "                      i,\n",
        "                      body_text,\n",
        "                      title_text,\n",
        "                      url,\n",
        "                      threshold):\n",
        "        \"\"\"\n",
        "        Prints an example of the model's prediction for manual inspection.\n",
        "        \"\"\"\n",
        "        if i:\n",
        "            print('\\n\\n==============================================')\n",
        "            print(f'============== Example # {i} =================\\n')\n",
        "        if url:\n",
        "            print(url)\n",
        "\n",
        "        print(f\"Issue Body:\\n {body_text} \\n\")\n",
        "\n",
        "        if title_text:\n",
        "            print(f\"Original Title:\\n {title_text}\")\n",
        "\n",
        "        emb, gen_title = self.generate_issue_title(body_text)\n",
        "        print(f\"\\n****** Machine Generated Title (Prediction) ******:\\n {gen_title}\")\n",
        "\n",
        "\n",
        "    def demo_model_predictions(self,n,issue_df,threshold=1):\n",
        "        \"\"\"\n",
        "        Pick n random Issues and display predictions.\n",
        "        Input:\n",
        "        ------\n",
        "        n : int\n",
        "            Number of issues to display from issue_df\n",
        "        issue_df : pandas DataFrame\n",
        "            DataFrame that contains two columns: `body` and `issue_title`.\n",
        "        threshold : float\n",
        "            distance threshold for recommendation of similar issues.\n",
        "        Returns:\n",
        "        --------\n",
        "        None\n",
        "            Prints the original issue body and the model's prediction.\n",
        "        \"\"\"\n",
        "        # Extract body and title from DF\n",
        "        body_text = issue_df.body.tolist()\n",
        "        title_text = issue_df.issue_title.tolist()\n",
        "        url = issue_df.issue_url.tolist()\n",
        "        demo_list = np.random.randint(low=1, high=len(body_text), size=n)\n",
        "        for i in demo_list:\n",
        "            self.print_example(i,\n",
        "                               body_text=body_text[i],\n",
        "                               title_text=title_text[i],\n",
        "                               url=url[i],\n",
        "                               threshold=threshold)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oskomViPMDzx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
        "                                 decoder_preprocessor=title_pp,\n",
        "                                 seq2seq_model=seq2seq_Model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tksmHQTvMDyS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6055
        },
        "outputId": "66afd7c4-b39c-478a-a5ae-b63beb6608c0"
      },
      "source": [
        "seq2seq_inf.demo_model_predictions(n=25, issue_df=testdf)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 1372 =================\n",
            "\n",
            "\"https://github.com/Yelp/elastalert/issues/855\"\n",
            "Issue Body:\n",
            " how do i know if it loaded my rules when running it? the elastalert_status index is empty and im using the following command python -m elastalert.elastalert --verbose --config /etc/elastalert/config.yaml . the file /etc/elastalert/config.yaml contains the row rules_folder: /etc/alestalert/rules and that folder contains 9 yaml files. the output i get in the console is: python -m elastalert.elastalert --verbose --config /etc/elastalert/config.yaml info:elastalert:sleeping for 299 seconds info:elastalert:sleeping for 299 seconds info:elastalert:sleeping for 299 seconds ... im using the es5 branch if that matters.. \n",
            "\n",
            "Original Title:\n",
            " elastalert does not create data in the index elastalert_status and no alerts seem to trigger?\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " how to get a separate file from a file\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 5855 =================\n",
            "\n",
            "\"https://github.com/AppStateESS/InternshipInventory/issues/344\"\n",
            "Issue Body:\n",
            " on our eventual configuration page, we need to add a setting for the email address that's linked to from the get help button in the menu bar. refs 323 \n",
            "\n",
            "Original Title:\n",
            " address for get help button should be configurable\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " add a way to change the user to a new tab\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 607 =================\n",
            "\n",
            "\"https://github.com/TabakoffLab/PhenoGen/issues/356\"\n",
            "Issue Body:\n",
            " view details in rollbar: https://rollbar.com/smahaffey/phenogen/items/2/ https://rollbar.com/smahaffey/phenogen/items/2/ typeerror: converting circular structure to json file https://phenogen.ucdenver.edu/phenogen/javascript/bugsense.2.2.1.min.js , line 2, in anonymous \n",
            "\n",
            "Original Title:\n",
            " typeerror: converting circular structure to json\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " error in json file\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 4392 =================\n",
            "\n",
            "\"https://github.com/openstreetcam/upload-scripts/issues/35\"\n",
            "Issue Body:\n",
            " hi, do you have a plan to develop a download api o something like that? \n",
            "\n",
            "Original Title:\n",
            " what about download-scripts?\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " implement a new project\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 1938 =================\n",
            "\n",
            "\"https://github.com/intel-analytics/BigDL/issues/1331\"\n",
            "Issue Body:\n",
            " https://github.com/intel-analytics/bigdl/tree/master/pyspark/bigdl/models/textclassifier only provide instructions for the manually installation, pip install users will not know where to find the bigdl home and jar files. \n",
            "\n",
            "Original Title:\n",
            " python textclassifier doc needs update for pip install\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " add support for the app\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 5071 =================\n",
            "\n",
            "\"https://github.com/stripe/safesql/issues/11\"\n",
            "Issue Body:\n",
            " i am researching tools that could detect leaked connections, e.g. failing to call rows.close after a call to db.query this tool does not do this right? should it? any suggestions for tools that are out there that might accept a pr if not this one? i want to avoid writing something from scratch. \n",
            "\n",
            "Original Title:\n",
            " detect connection leaks\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " can t connect to the server\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 3631 =================\n",
            "\n",
            "\"https://github.com/RRZE-Webteam/FAU-Einrichtungen/issues/319\"\n",
            "Issue Body:\n",
            " nur bei mobile devives <768: der sprung in den suchschlitz trifft für den fall, dass es einen weiteren suchschlitz in der seite gibt im widget , die falsche sucheingabemaske. dadurch verrutscht der fokus und der user ist verwirrt. \n",
            "\n",
            "Original Title:\n",
            " mobile: sprung in suche\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " add a feedback for the icon of the icon\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 2515 =================\n",
            "\n",
            "\"https://github.com/cyclejs/cyclejs/issues/640\"\n",
            "Issue Body:\n",
            " i see this error when building with v2.4.1, it works fine with 2.3.4 typescript error: node_modules/@cycle/dom/lib/maindomsource.d.ts 6,22 : error ts2420: class 'maindomsource' incorrectly implements interface 'domsource'. types of property 'select' are incompatible. type ' selector: string => domsource' is not assignable to type '<s extends domsource> selector: string => s'. type 'domsource' is not assignable to type 's'. \n",
            "\n",
            "Original Title:\n",
            " typescript compilation fails with v2.4.1\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " error when using a string with a string\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 4437 =================\n",
            "\n",
            "\"https://github.com/wduda/TravelWindow/issues/1\"\n",
            "Issue Body:\n",
            " comment from lotrointerface: > i just bought milestone skill vii, and i have a milestone bound to it, but when i use the popup screen for travel window, it says that return home 7 is not bound. anyone encounter this before? \n",
            "\n",
            "Original Title:\n",
            " travel skill vii not bound\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " can t be able to change the window\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 14 =================\n",
            "\n",
            "\"https://github.com/5-gwoap/adopt-a-family/issues/72\"\n",
            "Issue Body:\n",
            " need to click on a link on the root path https://test-aaf.herokuapp.com/ otherwise there is an error. e.g: https://test-aaf.herokuapp.com/dashboard \n",
            "\n",
            "Original Title:\n",
            " cannot load routes via direct url\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " link to a new file\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 3075 =================\n",
            "\n",
            "\"https://github.com/MidwestFurryFandom/rams/issues/69\"\n",
            "Issue Body:\n",
            " the price bump warning: 1. says no later than, which is a reference to bucket-based pricing. it should only say this if there is bucket-based pricing. 2. warns of price increases for the sponsor and shiny badges, whose prices do not increase. \n",
            "\n",
            "Original Title:\n",
            " as an attendee, i want price bump warnings to be accurate\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " bug report\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 1915 =================\n",
            "\n",
            "\"https://github.com/maldrasen/jadefire/issues/11\"\n",
            "Issue Body:\n",
            " not critical, but looking at the latest version, there's really no reason for the aspect and the mirrored aspect to share the same data object. we should just split all the aspects into their own objects when we have the time. \n",
            "\n",
            "Original Title:\n",
            " refactor aspects again\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " question about the same time\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 5917 =================\n",
            "\n",
            "\"https://github.com/batfish/batfish/issues/644\"\n",
            "Issue Body:\n",
            " the logic for prefix containment check used in virtualrouter.activatestaticroutes can be extracted and potentially re-used in other places. \n",
            "\n",
            "Original Title:\n",
            " extract a prefix.contains function\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " add a new class\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 3292 =================\n",
            "\n",
            "\"https://github.com/lstjsuperman/fabric/issues/31922\"\n",
            "Issue Body:\n",
            " in android.support.v4.view.viewpropertyanimatorcompatics.alpha number of crashes: 1 impacted devices: 1 there's a lot more information about this crash on crashlytics.com: https://fabric.io/momo6/android/apps/com.immomo.momo/issues/5a3619ee8cb3c2fa630f6c2e?utm_medium=service_hooks-github&utm_source=issue_impact https://fabric.io/momo6/android/apps/com.immomo.momo/issues/5a3619ee8cb3c2fa630f6c2e?utm_medium=service_hooks-github&utm_source=issue_impact \n",
            "\n",
            "Original Title:\n",
            " viewpropertyanimatorcompatics.java line 34\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " line number\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 1373 =================\n",
            "\n",
            "\"https://github.com/dgoodwin/tito/issues/274\"\n",
            "Issue Body:\n",
            " the --use-version option to tito tag is ignored if tito.tagger.releasetagger is used. tito version: $ rpm -q tito tito-0.6.10-1.el7.noarch .tito/tito.props: buildconfig builder = tito.builder.builder tagger = tito.tagger.releasetagger changelog_do_not_remove_cherrypick = 0 changelog_format = %s %ae steps to reproduce: - run tito tag --use-version 1.2.3 , and save changelog expected behavior: tito produces a tag matching <package-name>-1.2.3-<release>, or raises an error telling me it's invalid to use --use-version and releasetagger together. actual behavior: tito behaves the same as if i'd omitted --use-version option, and generates a tag by bumping the current release. additional notes: it works in tito 0.6.9. \n",
            "\n",
            "Original Title:\n",
            " tito tag --use-version option is ignored with releasetagger\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " update the version of the version of the version of the version\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 4891 =================\n",
            "\n",
            "\"https://github.com/persepolisdm/persepolis/issues/307\"\n",
            "Issue Body:\n",
            " system details: operating system: gnu/linux distro for gnu/linux and bsd users : ubuntu 16.04 desktop environment for gnu/linux and bsd users : gnome persepolis version: 2.4.2 how do you install persepolis? repositories it seem's queue doesn't have any download limits, like the single downloads. so if i start queue, i can't even open a web page. \n",
            "\n",
            "Original Title:\n",
            " queue doesn't have any download limits\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " how to configure the same folder in a single line\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 1472 =================\n",
            "\n",
            "\"https://github.com/freshertm/QFileManager/issues/2\"\n",
            "Issue Body:\n",
            " as a user i would have to copy and move files between panels using f5 and f6 keys for it. accetance - ability to copy selected files or directories from active panel to another using f5 key. - ability to move selected files or directories from active panel to another using f6 key. \n",
            "\n",
            "Original Title:\n",
            " implement copy and move file function\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " save the file to the file\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 191 =================\n",
            "\n",
            "\"https://github.com/Marcosrcruz/WMS-Cooper/issues/10\"\n",
            "Issue Body:\n",
            " campos id integer pk id_filial integer fk dataemissao date dataentrada date numerodocumento integer uk id_produto integer fk quantidade integer custounitario float valortotal float id_deposito integer fk lote integer uk lotevalidade integer \n",
            "\n",
            "Original Title:\n",
            " criar tabela entrada nf-e\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " add a string of the value of the value of the value\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 499 =================\n",
            "\n",
            "\"https://github.com/ManrajGrover/organize-cli/issues/16\"\n",
            "Issue Body:\n",
            " explain source and output switches in readme.md \n",
            "\n",
            "Original Title:\n",
            " explain source and output switches in readme\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " add a readme md\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 1694 =================\n",
            "\n",
            "\"https://github.com/haugene/docker-transmission-openvpn/issues/289\"\n",
            "Issue Body:\n",
            " i have a vpn set up from my work to home & my phone to home & with other transmissionvpn dockers have always had access to transmission webui. after installing this one, i can only access the webui when i am on my home network. is there anyway to gain access to the webui outside of my local network? on unraid with a couple other dockers i have had to install iptable_mangle /sbin/modprobe iptable_mangle but that doesn't seem to work with this docker container \n",
            "\n",
            "Original Title:\n",
            " no access to web ui outside local network ip's\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " can t be able to start with a new\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 3791 =================\n",
            "\n",
            "\"https://github.com/mozilla/network/issues/284\"\n",
            "Issue Body:\n",
            " let's remove all white icons currently in the cms. let's also archive in google drive to avoid confusion \n",
            "\n",
            "Original Title:\n",
            " fix icons on news\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " remove the icons from the same repository\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 430 =================\n",
            "\n",
            "\"https://github.com/zulip/zulipbot/issues/129\"\n",
            "Issue Body:\n",
            " currently, we just check for keywords and then automatically take into account the number as an issue. however, it may not necessarily be an issue, so make sure to check if it is an issue. example: https://github.com/zulip/zulip/pull/7333 \n",
            "\n",
            "Original Title:\n",
            " check commit message should make sure referenced number is an issue\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " make the default value for the same name\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 188 =================\n",
            "\n",
            "\"https://github.com/Indicia-Team/warehouse/issues/257\"\n",
            "Issue Body:\n",
            " having deleted a sample using the warehouse website, e.g. http://warehouse1.indicia.org.uk/index.php/sample/edit/2552642, it remains viewable as does its occurrence which was also marked deleted by the operation, http://warehouse1.indicia.org.uk/index.php/occurrence/edit/5097778. however, all the attribute information is removed from those views. my expectation was that deleted records would not be viewable. if they are to be viewable i would expect, as a matter of consistency, that their attribute information would also be presented. moreover, if they are to be viewable, an indication that you are looking at a deleted record might be useful. far from a priority but thought i'd log it. \n",
            "\n",
            "Original Title:\n",
            " unexpected viewing of deleted records.\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " bug with a new single line\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 1866 =================\n",
            "\n",
            "\"https://github.com/LesDrones/Drone/issues/5773\"\n",
            "Issue Body:\n",
            " <blockquote class= twitter-tweet >\n",
            "<p lang= fr dir= ltr xml:lang= fr >a quoi sert un bracelet connect&eacute; : <a href= https://t.co/vvxpns6xa6 >https://t.co/vvxpns6xa6</a></p>\n",
            "&mdash; les drones @les_drones <a href= https://twitter.com/les_drones/status/921368882803363841?ref_src=twsrc%5etfw >october 20, 2017</a>\n",
            "</blockquote> <br><br>\n",
            "october 20, 2017 at 03:33pm<br> \n",
            "\n",
            "Original Title:\n",
            " a quoi sert un bracelet connecté : https://t.co/vvxpns6xa6\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " le drone de l des drones url actu drone\n",
            "\n",
            "\n",
            "==============================================\n",
            "============== Example # 2509 =================\n",
            "\n",
            "\"https://github.com/pageboard/write/issues/2\"\n",
            "Issue Body:\n",
            " reproduce: 1. add sitemap 2. discard the page is emptied instead of restored. probably caused by serialization of current page in store. \n",
            "\n",
            "Original Title:\n",
            " corrupted store.initial after adding sitemap\n",
            "\n",
            "****** Machine Generated Title (Prediction) ******:\n",
            " create a page for the page\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_3FxjsMMDwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uauqe26yMDuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JPnEu0cMDsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE-Chwy7MDol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_Y2ECcXMDmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BK8t1taMDkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n-8i5jWMDiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjtFBQ5iMDf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTgRms4NMDdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFyIFMheMDaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}